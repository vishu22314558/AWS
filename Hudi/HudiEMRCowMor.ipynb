{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb15b35874be4504ba74882af61ede5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1610948431967_0002</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-213.ec2.internal:20888/proxy/application_1610948431967_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-90-42.ec2.internal:8042/node/containerlogs/container_1610948431967_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res5: Int = 4\n"
     ]
    }
   ],
   "source": [
    "//hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "//hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "//hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar\n",
    "\n",
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1610948431967_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-213.ec2.internal:20888/proxy/application_1610948431967_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-90-42.ec2.internal:8042/node/containerlogs/container_1610948431967_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1610948431967_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-213.ec2.internal:20888/proxy/application_1610948431967_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-90-42.ec2.internal:8042/node/containerlogs/container_1610948431967_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142cc72d31db416eb8ce913e3cb4f04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "outputBucket: String = emrpysparkhudi\n",
      "inputDataBucket: String = bdmtest0909\n",
      "hudiTableName: String = indian_food_review_cow\n",
      "hudiTableRecordKey: String = foodid\n",
      "hudiTablePath: String = s3://emrpysparkhudi/createdataset/indian_food_review_cow\n",
      "hudiTablePartitionColumn: String = diet\n",
      "hudiTablePrecombineKey: String = timestamp\n"
     ]
    }
   ],
   "source": [
    "//Initialize a Spark Session for Hudi\n",
    "\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "\n",
    "\n",
    "//Where to Store Your Hudi Table \n",
    "val outputBucket = \"emrpysparkhudi\"\n",
    "// input data set \n",
    "val inputDataBucket = \"bdmtest0909\"\n",
    "//Specify common DataSourceWriteOptions int a single hudiOption variable \n",
    "val hudiTableName = \"indian_food_review_cow\"\n",
    "val hudiTableRecordKey = \"foodid\"\n",
    "val hudiTablePath = \"s3://\"+outputBucket+\"/createdataset/\"+hudiTableName\n",
    "val hudiTablePartitionColumn = \"diet\"\n",
    "val hudiTablePrecombineKey = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ed79bdcbde45fd9262f771811ab769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [foodid: string, name: string ... 9 more fields]\n",
      "root\n",
      " |-- foodid: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- prep_time: string (nullable = true)\n",
      " |-- cook_time: string (nullable = true)\n",
      " |-- flavor_profile: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- timestamp: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read data from s3 \n",
    "val sourceData = (spark.read.option(\"header\",true).csv(\"s3://\"+inputDataBucket+\"/*\")\n",
    "             .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                 .cache())\n",
    "sourceData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5686612e0944a49c1a17c26fb930e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|foodid|          name|      diet|        state|flavor_profile| course|        state| timestamp|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|     1|    Balu shahi|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610948780|\n",
      "|     2|        Boondi|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1610948780|\n",
      "|     3|Gajar ka halwa|vegetarian|       Punjab|         sweet|dessert|       Punjab|1610948780|\n",
      "|     4|        Ghevar|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1610948780|\n",
      "|     5|   Gulab jamun|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610948780|\n",
      "|     6|        Imarti|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610948780|\n",
      "|     7|        Jalebi|vegetarian|Uttar Pradesh|         sweet|dessert|Uttar Pradesh|1610948780|\n",
      "|     8|    Kaju katli|vegetarian|           -1|         sweet|dessert|           -1|1610948780|\n",
      "|     9|      Kalakand|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610948780|\n",
      "|    10|         Kheer|vegetarian|           -1|         sweet|dessert|           -1|1610948780|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sourceData.select(\"foodid\",\"name\",\"diet\",\"state\",\"flavor_profile\",\"course\",\"state\",\"timestamp\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed27ca4eaa945dcb9dcdfc920a8923a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.partition_fields -> diet, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> indian_food_review_cow, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> foodid, hoodie.table.name -> indian_food_review_cow, hoodie.datasource.write.storage.type -> COPY_ON_WRITE, hoodie.datasource.write.partitionpath.field -> diet)\n"
     ]
    }
   ],
   "source": [
    "// Set up our Hudi Data Source Options\n",
    "val hudiOptions = Map[String,String](\n",
    "    HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "    //for this detaset use COPY_ON_WRITE storage strategy other option us MERGE_ON_READ\n",
    "    DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"COPY_ON_WRITE\", \n",
    "    //next three options configure what Hudi should use as its record key \n",
    "    DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"foodid\",\n",
    "    DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n",
    "    //for this data set, we specify that we want to sync metadata with Hive \n",
    "    DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "    DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY -> classOf[MultiPartKeysValueExtractor].getName\n",
    "    )\n",
    "\n",
    "//getName,\n",
    "//    \"hoodie.parquet.max.file.size\" -> String.valueOf(1024 * 1024 * 1024),\n",
    "//    \"hoodie.parquet.small.file.limit\" -> String.valueOf(64 * 1024 * 1024),\n",
    "//    \"hoodie.parquet.compression.ratio\" -> String.valueOf(0.5),\n",
    "//    \"hoodie.insert.shuffle.parallelism\" -> String.valueOf(2))\n",
    "\n",
    "\n",
    "// Write input data to Hudi \n",
    "\n",
    "(sourceData.write\n",
    "  .format(\"org.apache.hudi\")\n",
    " // Opreation  Key tells Hudi whether this is an Insert Upsert or Bulk Insert operation \n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f256eee84124f19abc1030e45bb0245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[default,indian_food_review_cow,false]\n"
     ]
    }
   ],
   "source": [
    "//hive\n",
    "\n",
    "// Show create tabel from Hive metastore \n",
    "spark.sql(\"SHOW TABLES\").collect.foreach(println)\n",
    "//show create table indian_food_review_cow;\n",
    "//show partitions  indian_food_review_cow;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c9cfb6ce164b17852c6fb8dd92392d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read Hudi data set from s3 \n",
    "\n",
    "val readOptimzedHudiViewDF = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "// take a look at our data ... \n",
    "readOptimzedHudiViewDF.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c6cf835b2e4afb930a2daf99a7ff72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsertdf: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "+-----+\n",
      "|state|\n",
      "+-----+\n",
      "|India|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// select row and update - India \n",
    "val upsertdf = (readOptimzedHudiViewDF.filter($\"state\" === -1).withColumn(\"state\",lit(\"India\")))\n",
    "\n",
    "(upsertdf.select(upsertdf(\"state\")).distinct).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0239813ac1d4f1eb2a5dc6fa186dc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF_updated: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|          India|      24|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// IMP - Before , if you wanted to update data in s3 ,, you have to read the old data , merge with the new data\n",
    "// and then overwrite the old data .. with Hudi , we can directly update the data in-Place\n",
    "\n",
    "(upsertdf.write\n",
    " .format(\"org.apache.hudi\")\n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    " .options(hudiOptions)\n",
    " .mode(SaveMode.Append)\n",
    " .save(hudiTablePath)\n",
    ")\n",
    "\n",
    "\n",
    "// read OptimizedHudiViewDF from updated data set \n",
    "\n",
    "val readOptimzedHudiViewDF_updated = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "readOptimzedHudiViewDF_updated.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b18efd7483d4d6ca161785525b63ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleteRowDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+--------------------+--------------+---------+---------+--------------+-----------+-----+------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|foodid|    name|         ingredients|          diet|prep_time|cook_time|flavor_profile|     course|state|region| timestamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+--------------------+--------------+---------+---------+--------------+-----------+-----+------+----------+\n",
      "|     20210118054701|20210118054701_0_252|               252|            vegetarian|839642a3-a59b-4d6...|   252| Bebinca|Coconut milk, egg...|    vegetarian|       20|       60|         sweet|    dessert|  Goa|  West|1610948780|\n",
      "|     20210118054701|20210118054701_0_255|               255|            vegetarian|839642a3-a59b-4d6...|   255|  Pinaca|Brown rice, fenne...|    vegetarian|       -1|       -1|         sweet|    dessert|  Goa|  West|1610948780|\n",
      "|     20210118054701| 20210118054701_1_27|               212|        non vegetarian|5e48df4b-5c13-452...|   212|Vindaloo|Chicken, coconut ...|non vegetarian|       10|       40|         spicy|main course|  Goa|  West|1610948780|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+--------------------+--------------+---------+---------+--------------+-----------+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// delete operation \n",
    "\n",
    "val deleteRowDF = readOptimzedHudiViewDF_updated.filter($\"state\" === \"Goa\");\n",
    "deleteRowDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5d70f834384fb5aa12a5ec0d54a962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF_postdelete: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|          India|      24|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "|      Telangana|       5|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(deleteRowDF.write\n",
    " .format(\"org.apache.hudi\")\n",
    " .options(hudiOptions)\n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    " .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\")\n",
    " .mode(SaveMode.Append)\n",
    " .save(hudiTablePath))\n",
    "\n",
    "//You can also hard delete data by setting OPERATION_OPT_KEY to DELETE_OPERATION_OPT_VAL to\n",
    "//remove all records in the dataset you submit. For instructions on performing soft deletes, \n",
    "//and for more information about deleting data stored in Hudi tables, see Deletes in the Apache Hudi documentation.\n",
    "\n",
    "// read OptimizedHudiViewDF from updated data set \n",
    "val readOptimzedHudiViewDF_postdelete = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "readOptimzedHudiViewDF_postdelete.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbaf86f78cce440c9b4693662a5faef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: Array[String] = Array(20210118054701, 20210118055624)\n"
     ]
    }
   ],
   "source": [
    "// Point in time SQL \n",
    "\n",
    "val commits = (spark.sql(\"\"\" select distinct(_hoodie_commit_time) as commitTime from \n",
    "indian_food_ro_table order by commitTime\"\"\").map(k=> k.getString(0)).take(50))\n",
    "\n",
    "//commits.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4a543016894ff4bfdc6a9fc1edf651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.hudi.DataSourceReadOptions\n",
      "beginTime: String = 0\n",
      "endTime: String = 20210118054701\n",
      "dataSet: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.hudi.DataSourceReadOptions\n",
    "val beginTime = \"0\"\n",
    "\n",
    "val endTime = commits(0)\n",
    "\n",
    "val dataSet = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     // Mark that we want to do an incremental query \n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime) \n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)  \n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)) \n",
    "\n",
    "dataSet.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c629f9b4ca3048f4baca9fa2ae5f316e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginTime: String = 20210118054701\n",
      "endTime: String = 20210118055624\n",
      "dataSet1: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+-----+--------+\n",
      "|state|count(1)|\n",
      "+-----+--------+\n",
      "|India|      24|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val beginTime = commits(0)\n",
    "\n",
    "val endTime = commits(1)\n",
    "\n",
    "val dataSet1 = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     // Mark that we want to do an incremental query \n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime) \n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)  \n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)) \n",
    "\n",
    "dataSet1.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152146b7fa884c9f8234f6356c119aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputBucket: String = emrpysparkhudi\n",
      "inputDataBucket: String = bdmtest0909\n",
      "hudiTableName: String = indian_food_review_mor\n",
      "hudiTableRecordKey: String = foodid\n",
      "hudiTablePath: String = s3://emrpysparkhudi/createdataset/indian_food_review_mor\n",
      "hudiTablePartitionColumn: String = diet\n",
      "hudiTablePrecombineKey: String = timestamp\n",
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.partition_fields -> diet, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> indian_food_review_mor, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> foodid, hoodie.table.name -> indian_food_review_mor, hoodie.datasource.write.storage.type -> MERGE_ON_READ, hoodie.datasource.write.partitionpath.field -> diet)\n",
      "sourceData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [foodid: string, name: string ... 9 more fields]\n",
      "root\n",
      " |-- foodid: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- prep_time: string (nullable = true)\n",
      " |-- cook_time: string (nullable = true)\n",
      " |-- flavor_profile: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- timestamp: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Using MoR\n",
    "\n",
    "\n",
    "val outputBucket = \"emrpysparkhudi\"\n",
    "val inputDataBucket = \"bdmtest0909\"\n",
    "val hudiTableName = \"indian_food_review_mor\"\n",
    "val hudiTableRecordKey = \"foodid\"\n",
    "val hudiTablePath = \"s3://\"+outputBucket+\"/createdataset/\"+hudiTableName\n",
    "val hudiTablePartitionColumn = \"diet\"\n",
    "val hudiTablePrecombineKey = \"timestamp\"\n",
    "\n",
    "\n",
    "val hudiOptions = Map[String,String](\n",
    "    HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "    //for this detaset use COPY_ON_WRITE storage strategy other option us MERGE_ON_READ\n",
    "    DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"MERGE_ON_READ\", \n",
    "    //next three options configure what Hudi should use as its record key \n",
    "    DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"foodid\",\n",
    "    DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n",
    "    //for this data set, we specify that we want to sync metadata with Hive \n",
    "    DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "    DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY -> classOf[MultiPartKeysValueExtractor].getName\n",
    "    )\n",
    "\n",
    "// read data from s3 \n",
    "val sourceData = (spark.read.option(\"header\",true).csv(\"s3://\"+inputDataBucket+\"/*\")\n",
    "             .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                 .cache())\n",
    "sourceData.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddd5cd12e9749169bd51f2983205197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|foodid|          name|      diet|        state|flavor_profile| course|        state| timestamp|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|     1|    Balu shahi|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610948780|\n",
      "|     2|        Boondi|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1610948780|\n",
      "|     3|Gajar ka halwa|vegetarian|       Punjab|         sweet|dessert|       Punjab|1610948780|\n",
      "|     4|        Ghevar|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1610948780|\n",
      "|     5|   Gulab jamun|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610948780|\n",
      "|     6|        Imarti|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610948780|\n",
      "|     7|        Jalebi|vegetarian|Uttar Pradesh|         sweet|dessert|Uttar Pradesh|1610948780|\n",
      "|     8|    Kaju katli|vegetarian|           -1|         sweet|dessert|           -1|1610948780|\n",
      "|     9|      Kalakand|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610948780|\n",
      "|    10|         Kheer|vegetarian|           -1|         sweet|dessert|           -1|1610948780|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sourceData.select(\"foodid\",\"name\",\"diet\",\"state\",\"flavor_profile\",\"course\",\"state\",\"timestamp\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178b7ce55b3042e8af584c6dea677c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Write input data to Hudi \n",
    "\n",
    "(sourceData.write\n",
    "  .format(\"org.apache.hudi\")\n",
    " // Opreation  Key tells Hudi whether this is an Insert Upsert or Bulk Insert operation \n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0573219ca24e59afb4af57a2b738b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[default,indian_food_review_cow,false]\n",
      "[default,indian_food_review_mor_ro,false]\n",
      "[default,indian_food_review_mor_rt,false]\n",
      "[,indian_food_ro_table,true]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1862d66d394803a3cda734d42eeda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+--------------------+---------+---------+--------------+-----------+------------+----------+----------+--------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|foodid|                name|         ingredients|prep_time|cook_time|flavor_profile|     course|       state|    region| timestamp|          diet|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+--------------------+---------+---------+--------------+-----------+------------+----------+----------+--------------+\n",
      "|     20210118061352|20210118061352_1_482|                65|        non vegetarian|ff9a034b-1dd1-456...|    65|          Maach Jhol|Fish, potol, toma...|       10|       40|         spicy|main course|       Assam|North East|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_483|                66|        non vegetarian|ff9a034b-1dd1-456...|    66|         Pork Bharta|Boiled pork, onio...|       -1|       -1|         spicy|main course|     Tripura|North East|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_484|                68|        non vegetarian|ff9a034b-1dd1-456...|    68|               Galho|Rice, axone, salt...|        5|       15|         spicy|main course|    Nagaland|North East|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_485|                76|        non vegetarian|ff9a034b-1dd1-456...|    76|             Biryani|Chicken thighs, b...|       30|      120|         spicy|main course|   Telangana|     South|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_486|                77|        non vegetarian|ff9a034b-1dd1-456...|    77|      Butter chicken|Chicken, greek yo...|       10|       35|         spicy|main course|NCT of Delhi|     North|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_487|                80|        non vegetarian|ff9a034b-1dd1-456...|    80|      Chicken razala|Chicken, dahi, se...|       10|       35|         spicy|main course| West Bengal|      East|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_488|                81|        non vegetarian|ff9a034b-1dd1-456...|    81|Chicken Tikka masala|Naan bread, tomat...|       10|       50|         spicy|main course|      Punjab|     North|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_489|                82|        non vegetarian|ff9a034b-1dd1-456...|    82|       Chicken Tikka|Chicken, whole wh...|      120|       45|         spicy|    starter|      Punjab|     North|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_490|               123|        non vegetarian|ff9a034b-1dd1-456...|   123|    Tandoori Chicken|Greek yogurt, gar...|      240|       30|         spicy|main course|      Punjab|     North|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_491|               124|        non vegetarian|ff9a034b-1dd1-456...|   124| Tandoori Fish Tikka|Chickpea flour, b...|      240|       30|         spicy|    starter|      Punjab|     North|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_492|               138|        non vegetarian|ff9a034b-1dd1-456...|   138|            Beef Fry|Beef, coconut, ga...|       10|       60|         spicy|main course|      Kerala|     South|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_493|               164|        non vegetarian|ff9a034b-1dd1-456...|   164|     Chicken Varuval|Meat curry powder...|       10|       35|         spicy|main course|  Tamil Nadu|     South|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_494|               168|        non vegetarian|ff9a034b-1dd1-456...|   168|         Kolim Jawla|Baingan, fish, co...|       -1|       -1|         spicy|main course| Maharashtra|      West|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_495|               173|        non vegetarian|ff9a034b-1dd1-456...|   173|          Bombil fry|Bombay duck, malv...|       -1|       -1|         spicy|main course| Maharashtra|      West|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_496|               212|        non vegetarian|ff9a034b-1dd1-456...|   212|            Vindaloo|Chicken, coconut ...|       10|       40|         spicy|main course|         Goa|      West|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_497|               224|        non vegetarian|ff9a034b-1dd1-456...|   224|         Kumol Sawul|Rice, eggs, carro...|       -1|       -1|         spicy|main course|       Assam|North East|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_498|               226|        non vegetarian|ff9a034b-1dd1-456...|   226|          Alu Pitika|Potatoes, mustard...|        5|       20|         spicy|main course|       Assam|North East|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_499|               227|        non vegetarian|ff9a034b-1dd1-456...|   227|         Masor tenga|Ridge gourd, fish...|       15|       25|         spicy|main course|       Assam|North East|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_500|               229|        non vegetarian|ff9a034b-1dd1-456...|   229|         Bilahi Maas|Potatoes, garam m...|       10|       20|            -1|main course|       Assam|North East|1610948780|non vegetarian|\n",
      "|     20210118061352|20210118061352_1_501|               230|        non vegetarian|ff9a034b-1dd1-456...|   230|          Black rice|Forbidden black r...|       -1|       -1|            -1|main course|     Manipur|North East|1610948780|non vegetarian|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+--------------------+---------+---------+--------------+-----------+------------+----------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from indian_food_review_mor_ro\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290d52aa043c42c4b78e61338db122ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select state , count(*) from indian_food_review_mor_ro group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924a70fd343d4739be7055f46cce927d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select state , count(*) from indian_food_review_mor_rt group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1a8bf7a00145dc813e866463d2e25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read Hudi data set from s3 \n",
    "\n",
    "val readOptimzedHudiViewDF = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "// take a look at our data ... \n",
    "readOptimzedHudiViewDF.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6297ec36dc944f5887017165f7dff6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsertdf: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "+-----+\n",
      "|state|\n",
      "+-----+\n",
      "|India|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// select row and update - India \n",
    "val upsertdf = (readOptimzedHudiViewDF.filter($\"state\" === -1).withColumn(\"state\",lit(\"India\")))\n",
    "\n",
    "(upsertdf.select(upsertdf(\"state\")).distinct).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8a5bc2710a45cc8b08cd0057e6cce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(upsertdf.write\n",
    " .format(\"org.apache.hudi\")\n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    " .options(hudiOptions)\n",
    " .mode(SaveMode.Append)\n",
    " .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfc2f4bc68c4091a431f02bdc718a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF_updated: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|          India|      24|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read OptimizedHudiViewDF from updated data set \n",
    "\n",
    "val readOptimzedHudiViewDF_updated = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "readOptimzedHudiViewDF_updated.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7687067407f3421fb0da24d320ebb7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|          India|      24|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select state , count(*) from indian_food_review_mor_rt group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b387050827594460a210a3c488a3740a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: Array[String] = Array(20210118061352, 20210118062628)\n",
      "res143: String = [Ljava.lang.String;@2987aa63\n"
     ]
    }
   ],
   "source": [
    "// Point in time SQL \n",
    "\n",
    "val commits = (spark.sql(\"\"\" select distinct(_hoodie_commit_time) as commitTime from \n",
    "indian_food_review_mor_rt order by commitTime\"\"\").map(k=> k.getString(0)).take(50))\n",
    "\n",
    "commits.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val beginTime = commits(0)\n",
    "\n",
    "//val endTime = commits(1)\n",
    "\n",
    "val dataSet1 = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     // Mark that we want to do an incremental query \n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, \"20210118061352\") \n",
    "     //.option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)  \n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)) \n",
    "\n",
    "dataSet1.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()\n",
    "\n",
    "//https://github.com/apache/hudi/issues/998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
